{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Predicción (CSV para leer mapa logístico estándar - bifurcación filtrando r)"
      ],
      "metadata": {
        "id": "vGqR7DrEgK5v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN0opRkuf-Pj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Cargar los datos de la serie temporal\n",
        "data = pd.read_csv('/content/feigenbaum_estandar_rx.csv')\n",
        "\n",
        "# Filtrar los datos para r > 3.57\n",
        "filtered_data = data[data['r'] > 3.57]\n",
        "\n",
        "# Verificar si los datos filtrados están vacíos\n",
        "if filtered_data.empty:\n",
        "    raise ValueError(\"No hay datos disponibles tras el filtrado. Comprueba los valores 'r' en el dataset.\")\n",
        "\n",
        "# Seleccionar los valores 'x' para la predicción\n",
        "x_values = filtered_data['x'].values.reshape(-1, 1)\n",
        "\n",
        "# Normalización usando Min-Max Scaling\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "x_values_scaled = scaler.fit_transform(x_values)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "train_size = int(len(x_values_scaled) * 0.8)\n",
        "train_data, test_data = x_values_scaled[:train_size], x_values_scaled[train_size:]\n",
        "\n",
        "# Preparar las secuencias de entrada y salida\n",
        "def create_sequences(data, sequence_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        X.append(data[i:i + sequence_length])\n",
        "        y.append(data[i + sequence_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "sequence_length = 100\n",
        "X_train, y_train = create_sequences(train_data, sequence_length)\n",
        "X_test, y_test = create_sequences(test_data, sequence_length)\n",
        "\n",
        "# Asegurar el reshaping de las entradas para ser [samples, time steps, features]\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Construir el modelo LSTM con Dropout\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=64, input_shape=(sequence_length, 1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
        "\n",
        "# Implementar Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Desescalar las predicciones y los valores de prueba al rango original\n",
        "predictions_rescaled = scaler.inverse_transform(predictions)\n",
        "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Evaluar el modelo con métricas adicionales\n",
        "mse = mean_squared_error(y_test_rescaled, predictions_rescaled)\n",
        "mae = mean_absolute_error(y_test_rescaled, predictions_rescaled)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n",
        "# Guardar las predicciones en un archivo .txt\n",
        "np.savetxt('predictions_lstm.txt', predictions_rescaled, fmt='%.18f')\n",
        "\n",
        "# Graficar la pérdida de entrenamiento y validación\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Graficar el MSE de entrenamiento y validación\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['mse'], label='Training MSE')\n",
        "plt.plot(history.history['val_mse'], label='Validation MSE')\n",
        "plt.title('Training and Validation MSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Graficar los valores predichos vs los reales\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test_rescaled, label='Actual')\n",
        "plt.plot(predictions_rescaled, label='Predicted')\n",
        "plt.title('Predicted vs Actual Values')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicción (TXT)"
      ],
      "metadata": {
        "id": "eZVLr1cAgTG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Cargar los datos de la serie temporal desde un archivo .txt\n",
        "data = np.loadtxt('/content/feigenbaum_cubico_x_multiplied.txt')\n",
        "\n",
        "# Verificar si los datos están vacíos\n",
        "if data.size == 0:\n",
        "    raise ValueError(\"No hay datos disponibles tras el filtrado. Comprueba los valores 'r' en el dataset.\")\n",
        "\n",
        "# Normalización usando Min-Max Scaling\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_scaled = scaler.fit_transform(data.reshape(-1, 1))\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "train_size = int(len(data_scaled) * 0.8)\n",
        "train_data, test_data = data_scaled[:train_size], data_scaled[train_size:]\n",
        "\n",
        "# Preparar las secuencias de entrada y salida\n",
        "def create_sequences(data, sequence_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        X.append(data[i:i + sequence_length])\n",
        "        y.append(data[i + sequence_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "sequence_length = 100\n",
        "X_train, y_train = create_sequences(train_data, sequence_length)\n",
        "X_test, y_test = create_sequences(test_data, sequence_length)\n",
        "\n",
        "# Asegurar el reshaping de las entradas para ser [samples, time steps, features]\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Construir el modelo LSTM con Dropout\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=64, input_shape=(sequence_length, 1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
        "\n",
        "# Implementar Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Desescalar las predicciones y los valores de prueba al rango original\n",
        "predictions_rescaled = scaler.inverse_transform(predictions)\n",
        "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Evaluar el modelo con métricas adicionales\n",
        "mse = mean_squared_error(y_test_rescaled, predictions_rescaled)\n",
        "mae = mean_absolute_error(y_test_rescaled, predictions_rescaled)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n",
        "# Guardar las predicciones en un archivo .txt\n",
        "np.savetxt('feigenbaum_cubico_x_multiplied_prediccion.txt', predictions_rescaled, fmt='%.18f')\n",
        "\n",
        "# Graficar la pérdida de entrenamiento y validación\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Graficar el MSE de entrenamiento y validación\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['mse'], label='Training MSE')\n",
        "plt.plot(history.history['val_mse'], label='Validation MSE')\n",
        "plt.title('Training and Validation MSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Graficar los valores predichos vs los reales\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test_rescaled, label='Actual')\n",
        "plt.plot(predictions_rescaled, label='Predicted')\n",
        "plt.title('Predicted vs Actual Values')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "17ETgX8tgSxy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}